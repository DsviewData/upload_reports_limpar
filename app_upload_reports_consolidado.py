import streamlit as st
import pandas as pd
import requests
from datetime import datetime
from io import BytesIO
from msal import ConfidentialClientApplication
import unicodedata
import logging
import re

# Configurar logging para debug
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# === CREDENCIAIS via st.secrets ===
try:
    CLIENT_ID = st.secrets["CLIENT_ID"]
    CLIENT_SECRET = st.secrets["CLIENT_SECRET"]
    TENANT_ID = st.secrets["TENANT_ID"]
    EMAIL_ONEDRIVE = st.secrets["EMAIL_ONEDRIVE"]
    SITE_ID = st.secrets["SITE_ID"]
    DRIVE_ID = st.secrets["DRIVE_ID"]
except KeyError as e:
    st.error(f"‚ùå Credencial n√£o encontrada: {e}")
    st.stop()

PASTA = "Documentos Compartilhados/LimparAuto/FontedeDados"

# === AUTENTICA√á√ÉO ===
@st.cache_data(ttl=3300)  # Cache por 55 minutos (token v√°lido por 1h)
def obter_token():
    """Obt√©m token de acesso para Microsoft Graph API"""
    try:
        app = ConfidentialClientApplication(
            CLIENT_ID,
            authority=f"https://login.microsoftonline.com/{TENANT_ID}",
            client_credential=CLIENT_SECRET
        )
        result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])
        
        if "access_token" not in result:
            error_desc = result.get("error_description", "Token n√£o obtido")
            st.error(f"‚ùå Falha na autentica√ß√£o: {error_desc}")
            return None
            
        return result["access_token"]
        
    except Exception as e:
        st.error(f"‚ùå Erro na autentica√ß√£o: {str(e)}")
        logger.error(f"Erro de autentica√ß√£o: {e}")
        return None

# === FUN√á√ïES AUXILIARES ===
def extrair_responsavel_da_planilha(df):
    """Extrai o nome do respons√°vel da planilha usando diferentes estrat√©gias"""
    responsavel_encontrado = None
    metodo_deteccao = None
    
    try:
        # Estrat√©gia 1: Procurar coluna 'RESPONS√ÅVEL' ou 'RESPONSAVEL'
        colunas_responsavel = [col for col in df.columns if 
                             any(term in col.upper() for term in ['RESPONS√ÅVEL', 'RESPONSAVEL', 'RESP'])]
        
        if colunas_responsavel:
            coluna = colunas_responsavel[0]
            valores_unicos = df[coluna].dropna().unique()
            if len(valores_unicos) > 0:
                # Pegar o valor mais comum
                responsavel_encontrado = df[coluna].value_counts().index[0]
                metodo_deteccao = f"Coluna '{coluna}'"
        
        # Estrat√©gia 2: Procurar em c√©lulas espec√≠ficas (primeira linha, primeiras colunas)
        if not responsavel_encontrado:
            # Verificar primeira linha em busca de padr√µes como "Respons√°vel:", "Nome:", etc.
            primeira_linha = df.iloc[0] if not df.empty else pd.Series()
            for col in df.columns[:5]:  # Verificar apenas primeiras 5 colunas
                valor = str(primeira_linha.get(col, '')).strip()
                if valor and len(valor) > 2 and any(char.isalpha() for char in valor):
                    # Verificar se parece com um nome (tem espa√ßo e letras)
                    if ' ' in valor and len(valor.split()) >= 2:
                        responsavel_encontrado = valor
                        metodo_deteccao = f"Primeira linha, coluna '{col}'"
                        break
        
        # Estrat√©gia 3: Procurar em c√©lulas que contenham padr√µes de nome
        if not responsavel_encontrado:
            for idx, row in df.head(10).iterrows():  # Verificar apenas primeiras 10 linhas
                for col in df.columns:
                    valor = str(row[col]).strip()
                    if (valor and len(valor) > 2 and 
                        ' ' in valor and 
                        len(valor.split()) >= 2 and
                        not valor.replace(' ', '').isdigit() and  # N√£o √© apenas n√∫meros
                        not '/' in valor and  # N√£o √© data
                        not valor.upper() in ['NAN', 'NULL', 'NONE']):
                        
                        # Verificar se parece com nome (mais de 50% letras)
                        letras = sum(c.isalpha() for c in valor)
                        if letras / len(valor) > 0.5:
                            responsavel_encontrado = valor
                            metodo_deteccao = f"Linha {idx+1}, coluna '{col}'"
                            break
                if responsavel_encontrado:
                    break
        
        # Limpar e validar o nome encontrado
        if responsavel_encontrado:
            # Remover caracteres especiais no in√≠cio/fim
            responsavel_encontrado = re.sub(r'^[^\w\s]+|[^\w\s]+$', '', responsavel_encontrado).strip()
            
            # Capitalizar adequadamente
            responsavel_encontrado = ' '.join(word.capitalize() for word in responsavel_encontrado.split())
            
            # Verificar se o nome √© v√°lido (pelo menos 2 palavras, cada uma com pelo menos 2 caracteres)
            palavras = responsavel_encontrado.split()
            if len(palavras) < 2 or any(len(palavra) < 2 for palavra in palavras):
                responsavel_encontrado = None
                metodo_deteccao = None
    
    except Exception as e:
        logger.error(f"Erro ao extrair respons√°vel: {e}")
        responsavel_encontrado = None
        metodo_deteccao = None
    
    return responsavel_encontrado, metodo_deteccao

def criar_pasta_se_nao_existir(caminho_pasta, token):
    """Cria pasta no OneDrive se n√£o existir"""
    try:
        # Dividir o caminho em partes
        partes = caminho_pasta.split('/')
        caminho_atual = ""
        
        for parte in partes:
            if not parte:
                continue
                
            caminho_anterior = caminho_atual
            caminho_atual = f"{caminho_atual}/{parte}" if caminho_atual else parte
            
            # Verificar se a pasta existe
            url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{caminho_atual}"
            headers = {"Authorization": f"Bearer {token}"}
            response = requests.get(url, headers=headers)
            
            if response.status_code == 404:
                # Criar pasta
                parent_url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root"
                if caminho_anterior:
                    parent_url += f":/{caminho_anterior}"
                parent_url += ":/children"
                
                create_body = {
                    "name": parte,
                    "folder": {},
                    "@microsoft.graph.conflictBehavior": "rename"
                }
                
                create_response = requests.post(
                    parent_url, 
                    headers={**headers, "Content-Type": "application/json"}, 
                    json=create_body
                )
                
                if create_response.status_code not in [200, 201]:
                    logger.warning(f"N√£o foi poss√≠vel criar pasta {parte}")
                    
    except Exception as e:
        logger.warning(f"Erro ao criar estrutura de pastas: {e}")

# === UPLOAD E BACKUP ===
def mover_arquivo_existente(nome_arquivo, token):
    """Move arquivo existente para backup antes de substituir"""
    try:
        url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{PASTA}/{nome_arquivo}"
        headers = {"Authorization": f"Bearer {token}"}
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            file_id = response.json().get("id")
            timestamp = datetime.now().strftime("%Y-%m-%d_%Hh%M")
            nome_base = nome_arquivo.replace(".xlsx", "")
            novo_nome = f"{nome_base}_backup_{timestamp}.xlsx"
            
            patch_url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/items/{file_id}"
            patch_body = {"name": novo_nome}
            patch_headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            patch_response = requests.patch(patch_url, headers=patch_headers, json=patch_body)
            
            if patch_response.status_code in [200, 201]:
                st.info(f"üíæ Backup criado: {novo_nome}")
            else:
                st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel criar backup do arquivo existente")
                
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Erro ao processar backup: {str(e)}")
        logger.error(f"Erro no backup: {e}")

def upload_onedrive(nome_arquivo, conteudo_arquivo, token):
    """Faz upload de arquivo para OneDrive"""
    try:
        # Garantir que a pasta existe
        pasta_arquivo = "/".join(nome_arquivo.split("/")[:-1]) if "/" in nome_arquivo else ""
        if pasta_arquivo:
            criar_pasta_se_nao_existir(f"{PASTA}/{pasta_arquivo}", token)
        
        # Fazer backup se arquivo j√° existir
        if "/" not in nome_arquivo:  # Arquivo na raiz
            mover_arquivo_existente(nome_arquivo, token)
        
        url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{PASTA}/{nome_arquivo}:/content"
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/octet-stream"
        }
        response = requests.put(url, headers=headers, data=conteudo_arquivo)
        
        return response.status_code in [200, 201], response.status_code, response.text
        
    except Exception as e:
        logger.error(f"Erro no upload: {e}")
        return False, 500, f"Erro interno: {str(e)}"

# === FUN√á√ïES DE CONSOLIDA√á√ÉO ===
def validar_dados_enviados(df, responsavel):
    """Valida os dados enviados pelo usu√°rio"""
    erros = []
    avisos = []
    linhas_invalidas_detalhes = []
    
    # Colunas esperadas do sistema
    COLUNAS_ESPERADAS = [
        'GRUPO', 'CONCESSION√ÅRIA', 'LOJA', 'MARCA', 'UF', 'RESPONS√ÅVEL', 
        'CONSULTORES', 'DATA', 'TMO - DUTO', 'TMO - FREIO', 'TMO - SANIT', 
        'TMO - VERNIZ', 'CX EVAP', 'TMO - TOTAL'
    ]
    
    # Validar respons√°vel
    if not responsavel or not responsavel.strip():
        erros.append("‚ö†Ô∏è O nome do respons√°vel √© obrigat√≥rio")
    elif len(responsavel.strip()) < 2:
        erros.append("‚ö†Ô∏è O nome do respons√°vel deve ter pelo menos 2 caracteres")
    elif len(responsavel.strip().split()) < 2:
        avisos.append("‚ö†Ô∏è Recomenda-se informar nome e sobrenome do respons√°vel")
    
    # Validar se DataFrame n√£o est√° vazio
    if df.empty:
        erros.append("‚ùå A planilha est√° vazia")
        return erros, avisos, linhas_invalidas_detalhes
    
    # Validar se existe coluna DATA
    if "DATA" not in df.columns:
        erros.append("‚ö†Ô∏è A planilha deve conter uma coluna 'DATA'")
        avisos.append("üìã Lembre-se: o arquivo deve ter uma aba chamada 'Vendas CTs' com a coluna 'DATA'")
    else:
        # Validar se as datas s√£o v√°lidas
        df_temp = df.copy()
        df_temp["DATA_CONVERTIDA"] = pd.to_datetime(df_temp["DATA"], errors="coerce")
        
        # Identificar linhas com datas inv√°lidas
        linhas_invalidas_mask = df_temp["DATA_CONVERTIDA"].isna()
        linhas_invalidas = df_temp[linhas_invalidas_mask]
        datas_validas = df_temp["DATA_CONVERTIDA"].notna().sum()
        
        if datas_validas == 0:
            erros.append("‚ùå Nenhuma data v√°lida encontrada na coluna 'DATA'")
        elif len(linhas_invalidas) > 0:
            # Preparar detalhes das linhas inv√°lidas para exibi√ß√£o posterior
            for idx, row in linhas_invalidas.iterrows():
                linha_excel = idx + 2  # +2 porque Excel come√ßa em 1 e tem cabe√ßalho
                valor_data = str(row["DATA"]) if pd.notna(row["DATA"]) else "VAZIO"
                linhas_invalidas_detalhes.append({
                    "Linha no Excel": linha_excel,
                    "Data Inv√°lida": valor_data
                })
            
            avisos.append(f"‚ö†Ô∏è {len(linhas_invalidas)} linhas com datas inv√°lidas ser√£o ignoradas na consolida√ß√£o")
    
    # Validar duplicatas na planilha enviada
    if not df.empty and "DATA" in df.columns:
        df_temp = df.copy()
        df_temp["DATA"] = pd.to_datetime(df_temp["DATA"], errors="coerce")
        df_temp = df_temp.dropna(subset=["DATA"])
        
        if not df_temp.empty:
            duplicatas = df_temp.duplicated(subset=["DATA"], keep=False).sum()
            if duplicatas > 0:
                avisos.append(f"‚ö†Ô∏è {duplicatas} linhas com datas duplicadas na planilha")
    
    # Validar colunas essenciais
    colunas_faltantes = [col for col in COLUNAS_ESPERADAS if col not in df.columns]
    colunas_importantes = ['GRUPO', 'CONCESSION√ÅRIA', 'LOJA', 'MARCA', 'UF']
    colunas_importantes_faltantes = [col for col in colunas_importantes if col not in df.columns]
    
    if colunas_importantes_faltantes:
        avisos.append(f"‚ö†Ô∏è Colunas importantes n√£o encontradas: {', '.join(colunas_importantes_faltantes)}")
    
    if colunas_faltantes:
        avisos.append(f"üìã Colunas que ser√£o criadas automaticamente: {', '.join(colunas_faltantes)}")
    
    # Validar colunas num√©ricas
    colunas_numericas = ['TMO - DUTO', 'TMO - FREIO', 'TMO - SANIT', 'TMO - VERNIZ', 'CX EVAP', 'TMO - TOTAL']
    for col in colunas_numericas:
        if col in df.columns:
            valores_nao_numericos = pd.to_numeric(df[col], errors='coerce').isna().sum()
            if valores_nao_numericos > 0:
                avisos.append(f"‚ö†Ô∏è {valores_nao_numericos} valores n√£o num√©ricos na coluna '{col}' ser√£o convertidos para 0")
    
    return erros, avisos, linhas_invalidas_detalhes

def processar_consolidacao(df_novo, responsavel, token):
    """Processa a consolida√ß√£o dos dados - Compara√ß√£o linha por linha por RESPONS√ÅVEL + DATA"""
    consolidado_nome = "Reports_Geral_Consolidado.xlsx"

    # Definir colunas esperadas para compara√ß√£o
    COLUNAS_ESPERADAS = [
        'GRUPO', 'CONCESSION√ÅRIA', 'LOJA', 'MARCA', 'UF', 'RESPONS√ÅVEL', 
        'CONSULTORES', 'DATA', 'TMO - DUTO', 'TMO - FREIO', 'TMO - SANIT', 
        'TMO - VERNIZ', 'CX EVAP', 'TMO - TOTAL'
    ]

    # 1. Baixar arquivo consolidado existente
    url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{PASTA}/{consolidado_nome}:/content"
    headers = {"Authorization": f"Bearer {token}"}
    
    with st.spinner("üì• Baixando arquivo consolidado existente..."):
        r = requests.get(url, headers=headers)

    if r.status_code == 200:
        try:
            df_consolidado = pd.read_excel(BytesIO(r.content))
            df_consolidado.columns = df_consolidado.columns.str.strip().str.upper()
            st.info(f"üìÇ Arquivo consolidado existente carregado ({len(df_consolidado)} registros)")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Erro ao ler arquivo consolidado existente: {e}")
            df_consolidado = pd.DataFrame()
    else:
        df_consolidado = pd.DataFrame()
        st.info("üìÇ Criando novo arquivo consolidado")

    # 2. Preparar dados novos
    df_novo = df_novo.copy()
    df_novo["RESPONS√ÅVEL"] = responsavel.strip()
    df_novo.columns = df_novo.columns.str.strip().str.upper()
    
    # Converter datas e remover linhas inv√°lidas
    df_novo["DATA"] = pd.to_datetime(df_novo["DATA"], errors="coerce")
    linhas_invalidas = df_novo["DATA"].isna().sum()
    df_novo = df_novo.dropna(subset=["DATA"])

    if df_novo.empty:
        st.error("‚ùå Nenhum registro v√°lido para consolidar")
        return False

    if linhas_invalidas > 0:
        st.info(f"üßπ {linhas_invalidas} linhas com datas inv√°lidas foram removidas")

    # Verificar se colunas esperadas existem na planilha enviada
    colunas_faltantes = [col for col in COLUNAS_ESPERADAS if col not in df_novo.columns]
    if colunas_faltantes:
        st.warning(f"‚ö†Ô∏è Colunas n√£o encontradas na planilha enviada: {', '.join(colunas_faltantes)}")
        # Adicionar colunas faltantes com valores vazios
        for col in colunas_faltantes:
            df_novo[col] = None

    # Calcular TMO - TOTAL se n√£o existir
    if 'TMO - TOTAL' not in df_novo.columns or df_novo['TMO - TOTAL'].isna().all():
        colunas_tmo = ['TMO - DUTO', 'TMO - FREIO', 'TMO - SANIT', 'TMO - VERNIZ']
        colunas_tmo_existentes = [col for col in colunas_tmo if col in df_novo.columns]
        
        if colunas_tmo_existentes:
            df_novo['TMO - TOTAL'] = 0
            for col in colunas_tmo_existentes:
                df_novo[col] = pd.to_numeric(df_novo[col], errors='coerce').fillna(0)
                df_novo['TMO - TOTAL'] += df_novo[col]
            st.info(f"‚úÖ Coluna 'TMO - TOTAL' calculada automaticamente")

    # 3. Consolida√ß√£o linha por linha
    registros_inseridos = 0
    registros_atualizados = 0
    registros_sem_alteracao = 0
    detalhes_operacoes = []
    
    with st.spinner("üîÑ Processando consolida√ß√£o linha por linha..."):
        if not df_consolidado.empty:
            # Converter datas do consolidado
            df_consolidado["DATA"] = pd.to_datetime(df_consolidado["DATA"], errors="coerce")
            df_consolidado = df_consolidado.dropna(subset=["DATA"])
            
            # Garantir que todas as colunas esperadas existem no consolidado
            for col in COLUNAS_ESPERADAS:
                if col not in df_consolidado.columns:
                    df_consolidado[col] = None
            
            # Processar cada linha do arquivo novo
            for idx, linha_nova in df_novo.iterrows():
                data_nova = linha_nova["DATA"]
                responsavel_novo = linha_nova["RESPONS√ÅVEL"]
                
                # Buscar linha correspondente no consolidado (RESPONS√ÅVEL + DATA)
                mask_correspondencia = (
                    (df_consolidado["DATA"].dt.normalize() == data_nova.normalize()) &
                    (df_consolidado["RESPONS√ÅVEL"].str.strip().str.upper() == responsavel_novo.strip().upper())
                )
                
                linhas_correspondentes = df_consolidado[mask_correspondencia]
                
                if linhas_correspondentes.empty:
                    # Registro n√£o existe - INSERIR
                    # Reorganizar linha com as colunas na ordem esperada
                    linha_organizada = {}
                    for col in COLUNAS_ESPERADAS:
                        linha_organizada[col] = linha_nova.get(col, None)
                    
                    nova_linha = pd.DataFrame([linha_organizada])
                    df_consolidado = pd.concat([df_consolidado, nova_linha], ignore_index=True)
                    registros_inseridos += 1
                    
                    detalhes_operacoes.append({
                        "Opera√ß√£o": "INSERIR",
                        "Data": data_nova.strftime("%d/%m/%Y"),
                        "Respons√°vel": responsavel_novo,
                        "Observa√ß√£o": "Novo registro"
                    })
                    
                else:
                    # Registro existe - VERIFICAR SE HOUVE ALTERA√á√ÉO
                    linha_existente = linhas_correspondentes.iloc[0]
                    index_existente = linhas_correspondentes.index[0]
                    
                    # Comparar apenas as colunas esperadas (exceto DATA e RESPONS√ÅVEL que j√° foram usadas para busca)
                    colunas_comparacao = [col for col in COLUNAS_ESPERADAS 
                                        if col not in ["DATA", "RESPONS√ÅVEL"]]
                    
                    houve_alteracao = False
                    campos_alterados = []
                    
                    for col in colunas_comparacao:
                        valor_novo = linha_nova.get(col, None)
                        valor_existente = linha_existente.get(col, None)
                        
                        # Normalizar valores para compara√ß√£o
                        if pd.isna(valor_novo) and pd.isna(valor_existente):
                            continue  # Ambos s√£o NaN - sem altera√ß√£o
                        elif pd.isna(valor_novo) or pd.isna(valor_existente):
                            houve_alteracao = True
                            campos_alterados.append(col)
                        else:
                            # Para campos num√©ricos, converter para float para compara√ß√£o
                            if col in ['TMO - DUTO', 'TMO - FREIO', 'TMO - SANIT', 'TMO - VERNIZ', 'CX EVAP', 'TMO - TOTAL']:
                                try:
                                    val_novo_num = float(pd.to_numeric(valor_novo, errors='coerce'))
                                    val_exist_num = float(pd.to_numeric(valor_existente, errors='coerce'))
                                    
                                    # Comparar com toler√¢ncia para valores float
                                    if abs(val_novo_num - val_exist_num) > 0.001:
                                        houve_alteracao = True
                                        campos_alterados.append(col)
                                except:
                                    # Se n√£o conseguir converter, comparar como string
                                    if str(valor_novo).strip() != str(valor_existente).strip():
                                        houve_alteracao = True
                                        campos_alterados.append(col)
                            else:
                                # Para campos de texto, comparar como string
                                if str(valor_novo).strip().upper() != str(valor_existente).strip().upper():
                                    houve_alteracao = True
                                    campos_alterados.append(col)
                    
                    if houve_alteracao:
                        # ATUALIZAR linha existente
                        for col in COLUNAS_ESPERADAS:
                            if col in linha_nova.index:
                                df_consolidado.loc[index_existente, col] = linha_nova[col]
                        
                        registros_atualizados += 1
                        detalhes_operacoes.append({
                            "Opera√ß√£o": "ATUALIZAR",
                            "Data": data_nova.strftime("%d/%m/%Y"),
                            "Respons√°vel": responsavel_novo,
                            "Observa√ß√£o": f"Campos alterados: {', '.join(campos_alterados[:3])}" + 
                                        ("..." if len(campos_alterados) > 3 else "")
                        })
                    else:
                        # Sem altera√ß√£o
                        registros_sem_alteracao += 1
                        detalhes_operacoes.append({
                            "Opera√ß√£o": "SEM ALTERA√á√ÉO",
                            "Data": data_nova.strftime("%d/%m/%Y"),
                            "Respons√°vel": responsavel_novo,
                            "Observa√ß√£o": "Dados id√™nticos"
                        })
                        
            df_final = df_consolidado.copy()
            
        else:
            # Arquivo consolidado n√£o existe - inserir todos os registros
            # Reorganizar com colunas na ordem esperada
            df_organizado = pd.DataFrame()
            for col in COLUNAS_ESPERADAS:
                df_organizado[col] = df_novo.get(col, None)
            
            df_final = df_organizado.copy()
            registros_inseridos = len(df_novo)
            st.info("üìÇ Primeiro envio - criando arquivo consolidado")

    # 4. Reorganizar colunas na ordem esperada
    colunas_finais = COLUNAS_ESPERADAS.copy()
    # Adicionar colunas extras que possam existir
    for col in df_final.columns:
        if col not in colunas_finais:
            colunas_finais.append(col)
    
    df_final = df_final.reindex(columns=colunas_finais)
    
    # 5. Ordenar e finalizar
    df_final = df_final.sort_values(["DATA", "RESPONS√ÅVEL"], na_position='last').reset_index(drop=True)
    
    # Adicionar metadados de controle
    df_final["√öLTIMA_ATUALIZA√á√ÉO"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Salvar em buffer
    buffer = BytesIO()
    with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
        df_final.to_excel(writer, index=False, sheet_name="Vendas CTs")
    buffer.seek(0)

    # 6. Salvar c√≥pia do arquivo enviado
    salvar_arquivo_enviado(df_novo, responsavel, token)
    
    # 7. Upload do arquivo consolidado
    with st.spinner("üì§ Enviando arquivo consolidado..."):
        sucesso, status, resposta = upload_onedrive(consolidado_nome, buffer.read(), token)

    if sucesso:
        st.success("‚úÖ Consolida√ß√£o realizada com sucesso!")
        
        # M√©tricas de resultado
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("üìä Total de registros", len(df_final))
        with col2:
            st.metric("‚ûï Inseridos", registros_inseridos)
        with col3:
            st.metric("üîÑ Atualizados", registros_atualizados)
        with col4:
            st.metric("‚û°Ô∏è Sem altera√ß√£o", registros_sem_alteracao)
        
        # Mostrar detalhes das opera√ß√µes
        if detalhes_operacoes:
            with st.expander("üìã Detalhes das Opera√ß√µes Realizadas"):
                df_detalhes = pd.DataFrame(detalhes_operacoes)
                st.dataframe(df_detalhes, use_container_width=True, hide_index=True)
        
        # Informa√ß√µes do per√≠odo
        if not df_novo.empty:
            data_min = df_novo["DATA"].min().strftime("%d/%m/%Y")
            data_max = df_novo["DATA"].max().strftime("%d/%m/%Y")
            st.info(f"üìÖ Per√≠odo processado: {data_min} at√© {data_max}")
        
        return True
    else:
        st.error(f"‚ùå Erro no upload: Status {status}")
        if status != 500:  # N√£o mostrar erro interno completo
            st.code(resposta)
        return False

def salvar_arquivo_enviado(df, responsavel, token):
    """Salva uma c√≥pia do arquivo enviado pelo respons√°vel"""
    try:
        if not df.empty and "DATA" in df.columns:
            data_base = df["DATA"].min()
            nome_pasta = f"Relatorios_Enviados/{data_base.strftime('%Y-%m')}"
            timestamp = datetime.now().strftime('%d-%m-%Y_%Hh%M')
            
            # Limpar nome do respons√°vel para uso em arquivo
            responsavel_limpo = "".join(c for c in responsavel.strip() if c.isalnum() or c in (' ', '-', '_')).strip()
            nome_arquivo = f"{nome_pasta}/{responsavel_limpo}_{timestamp}.xlsx"
            
            # Salvar arquivo
            buffer_envio = BytesIO()
            with pd.ExcelWriter(buffer_envio, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name="Vendas CTs")
            buffer_envio.seek(0)
            
            sucesso, _, _ = upload_onedrive(nome_arquivo, buffer_envio.read(), token)
            if sucesso:
                st.info(f"üíæ C√≥pia salva em: {nome_arquivo}")
            else:
                st.warning("‚ö†Ô∏è N√£o foi poss√≠vel salvar c√≥pia do arquivo enviado")
                
    except Exception as e:
        st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel salvar c√≥pia do arquivo: {e}")
        logger.error(f"Erro ao salvar arquivo enviado: {e}")