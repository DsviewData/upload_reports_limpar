import streamlit as st
import pandas as pd
import requests
from datetime import datetime, timedelta
from io import BytesIO
from msal import ConfidentialClientApplication
import unicodedata
import logging
import os
import json
import uuid
import time

# ===========================
# CONFIGURA√á√ïES DE VERS√ÉO - ATUALIZADO v2.3.0
# ===========================
APP_VERSION = "2.3.0"
VERSION_DATE = "2025-08-20"
CHANGELOG = {
    "2.3.0": {
        "date": "2025-08-20",
        "changes": [
            "üéØ CORRE√á√ÉO CR√çTICA: L√≥gica de consolida√ß√£o por M√äS/ANO",
            "üîß Resolve problema de altera√ß√£o de datas criando duplicatas",
            "üìÖ Consolida√ß√£o agora agrupa por RESPONS√ÅVEL + PER√çODO MENSAL",
            "üõ°Ô∏è Verifica√ß√£o de seguran√ßa aprimorada para per√≠odos",
            "üìä An√°lise pr√©-consolida√ß√£o atualizada para m√™s/ano",
            "üîç Logs detalhados por per√≠odo mensal",
            "‚ö° Performance melhorada no agrupamento",
            "‚úÖ Elimina√ß√£o definitiva de inconsist√™ncias temporais"
        ]
    },
    "2.2.4": {
        "date": "2025-08-14",
        "changes": [
            "üîß CORRE√á√ÉO CR√çTICA: L√≥gica de consolida√ß√£o reescrita",
            "üõ°Ô∏è Sistema de verifica√ß√£o de seguran√ßa implementado",
            "üìù Logs detalhados para monitoramento de consolida√ß√£o",
            "üìä An√°lise pr√©-consolida√ß√£o com previs√£o de impacto",
            "‚ö° Feedback visual melhorado durante processo",
            "üö® Alertas claros antes e durante consolida√ß√£o",
            "üéØ Corrigido problema de exclus√£o indevida de respons√°veis",
            "üìà M√©tricas em tempo real durante processamento"
        ]
    }
}

# ===========================
# CONFIGURA√á√ÉO DE LOGGING
# ===========================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ===========================
# CREDENCIAIS VIA ST.SECRETS
# ===========================
try:
    CLIENT_ID = st.secrets["CLIENT_ID"]
    CLIENT_SECRET = st.secrets["CLIENT_SECRET"]
    TENANT_ID = st.secrets["TENANT_ID"]
    EMAIL_ONEDRIVE = st.secrets["EMAIL_ONEDRIVE"]
    SITE_ID = st.secrets["SITE_ID"]
    DRIVE_ID = st.secrets["DRIVE_ID"]
except KeyError as e:
    st.error(f"‚ùå Credencial n√£o encontrada: {e}")
    st.stop()

# ===========================
# CONFIGURA√á√ÉO DE PASTAS
# ===========================
PASTA_CONSOLIDADO = "Documentos Compartilhados/LimparAuto/FontedeDados"
PASTA_ENVIOS_BACKUPS = "Documentos Compartilhados/PlanilhasEnviadas_Backups/LimparAuto"
PASTA = PASTA_CONSOLIDADO

# ===========================
# CONFIGURA√á√ÉO DO SISTEMA DE LOCK
# ===========================
ARQUIVO_LOCK = "sistema_lock.json"
TIMEOUT_LOCK_MINUTOS = 10

# ===========================
# AUTENTICA√á√ÉO
# ===========================
@st.cache_data(ttl=3300)
def obter_token():
    """Obt√©m token de acesso para Microsoft Graph API"""
    try:
        app = ConfidentialClientApplication(
            CLIENT_ID,
            authority=f"https://login.microsoftonline.com/{TENANT_ID}",
            client_credential=CLIENT_SECRET
        )
        result = app.acquire_token_for_client(scopes=["https://graph.microsoft.com/.default"])
        
        if "access_token" not in result:
            error_desc = result.get("error_description", "Token n√£o obtido")
            st.error(f"‚ùå Falha na autentica√ß√£o: {error_desc}")
            return None
            
        return result["access_token"]
        
    except Exception as e:
        st.error(f"‚ùå Erro na autentica√ß√£o: {str(e)}")
        logger.error(f"Erro de autentica√ß√£o: {e}")
        return None

# ===========================
# SISTEMA DE LOCK
# ===========================
def gerar_id_sessao():
    """Gera um ID √∫nico para a sess√£o atual"""
    if 'session_id' not in st.session_state:
        st.session_state.session_id = str(uuid.uuid4())[:8]
    return st.session_state.session_id

def verificar_lock_existente(token):
    """Verifica se existe um lock ativo no sistema"""
    try:
        url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{PASTA_CONSOLIDADO}/{ARQUIVO_LOCK}:/content"
        headers = {"Authorization": f"Bearer {token}"}
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            lock_data = response.json()
            timestamp_lock = datetime.fromisoformat(lock_data['timestamp'])
            agora = datetime.now()
            
            if agora - timestamp_lock > timedelta(minutes=TIMEOUT_LOCK_MINUTOS):
                logger.info(f"Lock expirado removido automaticamente. Era de {timestamp_lock}")
                remover_lock(token, force=True)
                return False, None
            
            return True, lock_data
        
        elif response.status_code == 404:
            return False, None
        else:
            logger.warning(f"Erro ao verificar lock: {response.status_code}")
            return False, None
            
    except Exception as e:
        logger.error(f"Erro ao verificar lock: {e}")
        return False, None

def criar_lock(token, operacao="Consolida√ß√£o de dados"):
    """Cria um lock para bloquear outras opera√ß√µes"""
    try:
        session_id = gerar_id_sessao()
        
        lock_data = {
            "timestamp": datetime.now().isoformat(),
            "session_id": session_id,
            "operacao": operacao,
            "status": "EM_ANDAMENTO",
            "app_version": APP_VERSION
        }
        
        url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{PASTA_CONSOLIDADO}/{ARQUIVO_LOCK}:/content"
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        response = requests.put(url, headers=headers, data=json.dumps(lock_data))
        
        if response.status_code in [200, 201]:
            logger.info(f"Lock criado com sucesso. Session ID: {session_id}")
            return True, session_id
        else:
            logger.error(f"Erro ao criar lock: {response.status_code}")
            return False, None
            
    except Exception as e:
        logger.error(f"Erro ao criar lock: {e}")
        return False, None

def remover_lock(token, session_id=None, force=False):
    """Remove o lock do sistema"""
    try:
        if not force and session_id:
            lock_existe, lock_data = verificar_lock_existente(token)
            if lock_existe and lock_data.get('session_id') != session_id:
                logger.warning("Tentativa de remover lock de outra sess√£o!")
                return False
        
        url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{PASTA_CONSOLIDADO}/{ARQUIVO_LOCK}"
        headers = {"Authorization": f"Bearer {token}"}
        response = requests.delete(url, headers=headers)
        
        if response.status_code in [200, 204]:
            logger.info("Lock removido com sucesso")
            return True
        elif response.status_code == 404:
            return True
        else:
            logger.error(f"Erro ao remover lock: {response.status_code}")
            return False
            
    except Exception as e:
        logger.error(f"Erro ao remover lock: {e}")
        return False

def atualizar_status_lock(token, session_id, novo_status, detalhes=None):
    """Atualiza o status do lock durante o processo"""
    try:
        lock_existe, lock_data = verificar_lock_existente(token)
        
        if not lock_existe or lock_data.get('session_id') != session_id:
            logger.warning("Lock n√£o existe ou n√£o pertence a esta sess√£o")
            return False
        
        lock_data['status'] = novo_status
        lock_data['ultima_atualizacao'] = datetime.now().isoformat()
        
        if detalhes:
            lock_data['detalhes'] = detalhes
        
        url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{PASTA_CONSOLIDADO}/{ARQUIVO_LOCK}:/content"
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/json"
        }
        
        response = requests.put(url, headers=headers, data=json.dumps(lock_data))
        return response.status_code in [200, 201]
        
    except Exception as e:
        logger.error(f"Erro ao atualizar status do lock: {e}")
        return False

def exibir_status_sistema(token):
    """Exibe o status atual do sistema de lock"""
    lock_existe, lock_data = verificar_lock_existente(token)
    
    if lock_existe:
        timestamp_inicio = datetime.fromisoformat(lock_data['timestamp'])
        duracao = datetime.now() - timestamp_inicio
        
        if duracao.total_seconds() < 60:
            cor = "üü°"
        elif duracao.total_seconds() < 300:
            cor = "üü†"
        else:
            cor = "üî¥"
        
        tempo_limite = timestamp_inicio + timedelta(minutes=TIMEOUT_LOCK_MINUTOS)
        tempo_restante = tempo_limite - datetime.now()
        
        st.error(f"üîí **Sistema ocupado** - Outro usu√°rio est√° enviando dados")
        
        with st.expander("‚ÑπÔ∏è Detalhes do processo em andamento"):
            col1, col2 = st.columns(2)
            
            with col1:
                st.info(f"**Opera√ß√£o:** {lock_data.get('operacao', 'N/A')}")
                st.info(f"**Status:** {lock_data.get('status', 'N/A')}")
                st.info(f"**In√≠cio:** {timestamp_inicio.strftime('%H:%M:%S')}")
                
            with col2:
                st.info(f"{cor} **Dura√ß√£o:** {int(duracao.total_seconds()//60)}min {int(duracao.total_seconds()%60)}s")
                if tempo_restante.total_seconds() > 0:
                    st.info(f"‚è±Ô∏è **Timeout em:** {int(tempo_restante.total_seconds()//60)}min")
                else:
                    st.warning("‚ö†Ô∏è **Processo pode ter travado** (ser√° liberado automaticamente)")
                
            if 'detalhes' in lock_data:
                st.info(f"**Detalhes:** {lock_data['detalhes']}")
                
            session_id_display = lock_data.get('session_id', 'N/A')[:8]
            st.caption(f"Session ID: {session_id_display}")
        
        if tempo_restante.total_seconds() < 0:
            if st.button("üÜò Liberar Sistema (For√ßar)", type="secondary"):
                if remover_lock(token, force=True):
                    st.success("‚úÖ Sistema liberado com sucesso!")
                    st.rerun()
                else:
                    st.error("‚ùå Erro ao liberar sistema")
        
        return True
    else:
        st.success("‚úÖ **Sistema dispon√≠vel** - Voc√™ pode enviar sua planilha")
        return False

# ===========================
# FUN√á√ïES AUXILIARES
# ===========================
def criar_pasta_se_nao_existir(caminho_pasta, token):
    """Cria pasta no OneDrive se n√£o existir"""
    try:
        partes = caminho_pasta.split('/')
        caminho_atual = ""
        
        for parte in partes:
            if not parte:
                continue
                
            caminho_anterior = caminho_atual
            caminho_atual = f"{caminho_atual}/{parte}" if caminho_atual else parte
            
            url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{caminho_atual}"
            headers = {"Authorization": f"Bearer {token}"}
            response = requests.get(url, headers=headers)
            
            if response.status_code == 404:
                parent_url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root"
                if caminho_anterior:
                    parent_url += f":/{caminho_anterior}"
                parent_url += ":/children"
                
                create_body = {
                    "name": parte,
                    "folder": {},
                    "@microsoft.graph.conflictBehavior": "rename"
                }
                
                create_response = requests.post(
                    parent_url, 
                    headers={**headers, "Content-Type": "application/json"}, 
                    json=create_body
                )
                
                if create_response.status_code not in [200, 201]:
                    logger.warning(f"N√£o foi poss√≠vel criar pasta {parte}")
                    
    except Exception as e:
        logger.warning(f"Erro ao criar estrutura de pastas: {e}")

def upload_onedrive(nome_arquivo, conteudo_arquivo, token, tipo_arquivo="consolidado"):
    """Faz upload de arquivo para OneDrive"""
    try:
        if tipo_arquivo == "consolidado":
            pasta_base = PASTA_CONSOLIDADO
        elif tipo_arquivo in ["enviado", "backup"]:
            pasta_base = PASTA_ENVIOS_BACKUPS
        else:
            pasta_base = PASTA_CONSOLIDADO
        
        pasta_arquivo = "/".join(nome_arquivo.split("/")[:-1]) if "/" in nome_arquivo else ""
        if pasta_arquivo:
            criar_pasta_se_nao_existir(f"{pasta_base}/{pasta_arquivo}", token)
        
        if tipo_arquivo == "consolidado" and "/" not in nome_arquivo:
            mover_arquivo_existente(nome_arquivo, token, pasta_base)
        
        url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{pasta_base}/{nome_arquivo}:/content"
        headers = {
            "Authorization": f"Bearer {token}",
            "Content-Type": "application/octet-stream"
        }
        response = requests.put(url, headers=headers, data=conteudo_arquivo)
        
        return response.status_code in [200, 201], response.status_code, response.text
        
    except Exception as e:
        logger.error(f"Erro no upload: {e}")
        return False, 500, f"Erro interno: {str(e)}"

def mover_arquivo_existente(nome_arquivo, token, pasta_base=None):
    """Move arquivo existente para backup antes de substituir"""
    try:
        if pasta_base is None:
            pasta_base = PASTA_CONSOLIDADO
            
        url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{pasta_base}/{nome_arquivo}"
        headers = {"Authorization": f"Bearer {token}"}
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            file_id = response.json().get("id")
            timestamp = datetime.now().strftime("%Y-%m-%d_%Hh%M")
            nome_base = nome_arquivo.replace(".xlsx", "")
            novo_nome = f"{nome_base}_backup_{timestamp}.xlsx"
            
            patch_url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/items/{file_id}"
            patch_body = {"name": novo_nome}
            patch_headers = {
                "Authorization": f"Bearer {token}",
                "Content-Type": "application/json"
            }
            patch_response = requests.patch(patch_url, headers=patch_headers, json=patch_body)
            
            if patch_response.status_code in [200, 201]:
                st.info(f"üíæ Backup criado: {novo_nome}")
            else:
                st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel criar backup do arquivo existente")
                
    except Exception as e:
        st.warning(f"‚ö†Ô∏è Erro ao processar backup: {str(e)}")
        logger.error(f"Erro no backup: {e}")

# ===========================
# VALIDA√á√ÉO DE DATAS
# ===========================
def validar_datas_detalhadamente(df):
    """Valida√ß√£o detalhada de datas"""
    problemas = []
    
    logger.info(f"üîç Iniciando valida√ß√£o detalhada de {len(df)} registros...")
    
    for idx, row in df.iterrows():
        linha_excel = idx + 2
        valor_original = row["DATA"]
        responsavel = row.get("RESPONS√ÅVEL", "N/A")
        
        problema_encontrado = None
        tipo_problema = None
        
        if pd.isna(valor_original) or str(valor_original).strip() == "":
            problema_encontrado = "Data vazia ou nula"
            tipo_problema = "VAZIO"
        else:
            try:
                data_convertida = pd.to_datetime(valor_original, errors='raise')
                hoje = datetime.now()
                
                if data_convertida > hoje + pd.Timedelta(days=730):
                    problema_encontrado = f"Data muito distante no futuro: {data_convertida.strftime('%d/%m/%Y')}"
                    tipo_problema = "FUTURO"
                elif data_convertida < pd.Timestamp('2020-01-01'):
                    problema_encontrado = f"Data muito antiga: {data_convertida.strftime('%d/%m/%Y')}"
                    tipo_problema = "ANTIGA"
                elif data_convertida.day > 31 or data_convertida.month > 12:
                    problema_encontrado = f"Data imposs√≠vel: dia={data_convertida.day}, m√™s={data_convertida.month}"
                    tipo_problema = "IMPOSS√çVEL"
                elif data_convertida > hoje + pd.Timedelta(days=90):
                    problema_encontrado = f"Data no futuro (verificar se est√° correta): {data_convertida.strftime('%d/%m/%Y')}"
                    tipo_problema = "FUTURO"
                    
            except (ValueError, TypeError, pd.errors.OutOfBoundsDatetime, OverflowError):
                problema_encontrado = f"Formato inv√°lido: '{str(valor_original)}'"
                tipo_problema = "FORMATO"
        
        if problema_encontrado:
            problemas.append({
                "Linha no Excel": linha_excel,
                "Data Inv√°lida": str(valor_original)[:50],
                "Tipo Problema": tipo_problema,
                "Descri√ß√£o": problema_encontrado,
                "Respons√°vel": str(responsavel)[:30]
            })
    
    logger.info(f"‚úÖ Valida√ß√£o conclu√≠da: {len(problemas)} problemas encontrados")
    return problemas

def exibir_relatorio_problemas_datas(problemas_datas):
    """Exibe relat√≥rio visual detalhado dos problemas"""
    if not problemas_datas:
        st.success("‚úÖ **Todas as datas est√£o v√°lidas e consistentes!**")
        return
    
    st.error(f"‚ö†Ô∏è **ATEN√á√ÉO: {len(problemas_datas)} problemas encontrados nas datas**")
    
    df_problemas = pd.DataFrame(problemas_datas)
    
    if "Tipo Problema" in df_problemas.columns:
        tipos_problema = df_problemas.groupby('Tipo Problema').size().sort_values(ascending=False)
        
        st.markdown("### üìä **Resumo dos Problemas:**")
        
        cols = st.columns(min(len(tipos_problema), 4))
        
        emoji_map = {
            "VAZIO": "üî¥",
            "FORMATO": "üü†", 
            "IMPOSS√çVEL": "üü£",
            "FUTURO": "üü°",
            "ANTIGA": "üü§",
            "INCONSISTENTE": "‚ö´"
        }
        
        for i, (tipo, qtd) in enumerate(tipos_problema.items()):
            emoji = emoji_map.get(tipo, "‚ùå")
            col_idx = i % len(cols)
            
            with cols[col_idx]:
                st.metric(
                    label=f"{emoji} {tipo}",
                    value=f"{qtd} linha{'s' if qtd > 1 else ''}",
                    help=f"Problemas do tipo {tipo}"
                )
    
    st.divider()
    
    st.markdown("### üìã **Detalhes das Linhas com Problemas:**")
    
    colunas_exibir = ["Linha no Excel", "Respons√°vel", "Data Inv√°lida", "Descri√ß√£o"]
    max_linhas_exibir = 50
    
    if len(df_problemas) <= max_linhas_exibir:
        st.dataframe(
            df_problemas[colunas_exibir], 
            use_container_width=True, 
            hide_index=True,
            height=min(400, len(df_problemas) * 35)
        )
    else:
        st.dataframe(
            df_problemas.head(max_linhas_exibir)[colunas_exibir], 
            use_container_width=True, 
            hide_index=True,
            height=400
        )
        st.warning(f"‚ö†Ô∏è **Exibindo apenas as primeiras {max_linhas_exibir} linhas.** Total de problemas: {len(df_problemas)}")

def validar_dados_enviados(df):
    """Valida√ß√£o super rigorosa dos dados enviados"""
    erros = []
    avisos = []
    linhas_invalidas_detalhes = []
    
    if df.empty:
        erros.append("‚ùå A planilha est√° vazia")
        return erros, avisos, linhas_invalidas_detalhes
    
    if "RESPONS√ÅVEL" not in df.columns:
        erros.append("‚ö†Ô∏è A planilha deve conter uma coluna 'RESPONS√ÅVEL'")
        avisos.append("üìã Certifique-se de que sua planilha tenha uma coluna chamada 'RESPONS√ÅVEL'")
    else:
        responsaveis_validos = df["RESPONS√ÅVEL"].notna().sum()
        if responsaveis_validos == 0:
            erros.append("‚ùå Nenhum respons√°vel v√°lido encontrado na coluna 'RESPONS√ÅVEL'")
        else:
            responsaveis_unicos = df["RESPONS√ÅVEL"].dropna().unique()
            if len(responsaveis_unicos) > 0:
                avisos.append(f"üë• Respons√°veis encontrados: {', '.join(responsaveis_unicos[:5])}")
                if len(responsaveis_unicos) > 5:
                    avisos.append(f"... e mais {len(responsaveis_unicos) - 5} respons√°veis")
    
    if "DATA" not in df.columns:
        erros.append("‚ö†Ô∏è A planilha deve conter uma coluna 'DATA'")
        avisos.append("üìã Lembre-se: o arquivo deve ter uma aba chamada 'Vendas CTs' com as colunas 'DATA' e 'RESPONS√ÅVEL'")
    else:
        problemas_datas = validar_datas_detalhadamente(df)
        
        if problemas_datas:
            erros.append(f"‚ùå {len(problemas_datas)} problemas de data encontrados - CONSOLIDA√á√ÉO BLOQUEADA")
            erros.append("üîß √â OBRIGAT√ìRIO corrigir TODOS os problemas antes de enviar")
            erros.append("üìã Revise sua planilha e corrija todas as datas inv√°lidas")
            
            tipos_problema = {}
            for problema in problemas_datas:
                tipo = problema["Tipo Problema"]
                tipos_problema[tipo] = tipos_problema.get(tipo, 0) + 1
            
            detalhes_problemas = []
            emoji_map = {
                "VAZIO": "üî¥",
                "FORMATO": "üü†", 
                "IMPOSS√çVEL": "üü£",
                "FUTURO": "üü°",
                "ANTIGA": "üü§"
            }
            
            for tipo, qtd in tipos_problema.items():
                emoji = emoji_map.get(tipo, "‚ùå")
                detalhes_problemas.append(f"{emoji} {tipo}: {qtd} linha{'s' if qtd > 1 else ''}")
            
            erros.append(f"üìä Problemas por tipo: {', '.join(detalhes_problemas)}")
            
            if "VAZIO" in tipos_problema:
                erros.append("üî¥ CR√çTICO: Existem datas em branco - preencha todas as datas")
            if "FORMATO" in tipos_problema:
                erros.append("üü† CR√çTICO: Existem formatos inv√°lidos - use formato DD/MM/AAAA")
            if "IMPOSS√çVEL" in tipos_problema:
                erros.append("üü£ CR√çTICO: Existem datas imposs√≠veis - verifique dias e meses")
            if "FUTURO" in tipos_problema:
                erros.append("üü° ATEN√á√ÉO: Existem datas no futuro - confirme se est√£o corretas")
            if "ANTIGA" in tipos_problema:
                erros.append("üü§ ATEN√á√ÉO: Existem datas muito antigas - confirme se est√£o corretas")
            
            linhas_invalidas_detalhes = problemas_datas
            
        else:
            avisos.append("‚úÖ Todas as datas est√£o v√°lidas e consistentes!")
    
    if not df.empty and "DATA" in df.columns:
        df_temp = df.copy()
        df_temp["DATA"] = pd.to_datetime(df_temp["DATA"], errors="coerce")
        df_temp = df_temp.dropna(subset=["DATA"])
        
        if not df_temp.empty:
            duplicatas = df_temp.duplicated(subset=["DATA"], keep=False).sum()
            if duplicatas > 0:
                avisos.append(f"‚ö†Ô∏è {duplicatas} linhas com datas duplicadas na planilha")
    
    return erros, avisos, linhas_invalidas_detalhes

# ===========================
# FUN√á√ïES DE CONSOLIDA√á√ÉO CORRIGIDAS v2.3.0
# ===========================
def baixar_arquivo_consolidado(token):
    """Baixa o arquivo consolidado existente"""
    consolidado_nome = "Reports_Geral_Consolidado.xlsx"
    url = f"https://graph.microsoft.com/v1.0/sites/{SITE_ID}/drives/{DRIVE_ID}/root:/{PASTA_CONSOLIDADO}/{consolidado_nome}:/content"
    headers = {"Authorization": f"Bearer {token}"}
    
    try:
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            df_consolidado = pd.read_excel(BytesIO(response.content))
            df_consolidado.columns = df_consolidado.columns.str.strip().str.upper()
            
            logger.info(f"‚úÖ Arquivo consolidado baixado: {len(df_consolidado)} registros")
            if not df_consolidado.empty:
                responsaveis_existentes = df_consolidado['RESPONS√ÅVEL'].dropna().unique()
                logger.info(f"üìä Respons√°veis no consolidado: {responsaveis_existentes}")
            
            return df_consolidado, True
        else:
            logger.info("üìÑ Arquivo consolidado n√£o existe - ser√° criado novo")
            return pd.DataFrame(), False
            
    except Exception as e:
        logger.error(f"Erro ao baixar arquivo consolidado: {e}")
        return pd.DataFrame(), False

def verificar_seguranca_consolidacao_v2(df_consolidado, df_novo, df_final):
    """Verifica√ß√£o de seguran√ßa cr√≠tica - vers√£o corrigida para m√™s/ano"""
    try:
        responsaveis_antes = set(df_consolidado['RESPONS√ÅVEL'].dropna().astype(str).str.strip().str.upper().unique()) if not df_consolidado.empty else set()
        responsaveis_novos = set(df_novo['RESPONS√ÅVEL'].dropna().astype(str).str.strip().str.upper().unique())
        responsaveis_depois = set(df_final['RESPONS√ÅVEL'].dropna().astype(str).str.strip().str.upper().unique())
        
        logger.info(f"üõ°Ô∏è VERIFICA√á√ÉO DE SEGURAN√áA v2.3.0:")
        logger.info(f"   Respons√°veis ANTES: {responsaveis_antes}")
        logger.info(f"   Respons√°veis NOVOS: {responsaveis_novos}")
        logger.info(f"   Respons√°veis DEPOIS: {responsaveis_depois}")
        
        # Verificar se algum respons√°vel foi perdido completamente
        responsaveis_esperados = responsaveis_antes.union(responsaveis_novos)
        responsaveis_perdidos = responsaveis_esperados - responsaveis_depois
        
        if responsaveis_perdidos:
            error_msg = f"Respons√°veis perdidos durante consolida√ß√£o: {', '.join(responsaveis_perdidos)}"
            logger.error(f"‚ùå ERRO CR√çTICO: {error_msg}")
            return False, error_msg
        
        # Verifica√ß√£o adicional: respons√°veis que n√£o est√£o sendo atualizados n√£o devem desaparecer
        for resp in responsaveis_antes:
            if resp not in responsaveis_novos:
                if resp not in responsaveis_depois:
                    error_msg = f"Respons√°vel '{resp}' foi removido sem justificativa"
                    logger.error(f"‚ùå ERRO: {error_msg}")
                    return False, error_msg
        
        # Verifica√ß√£o de per√≠odos: garantir que n√£o perdemos dados de per√≠odos n√£o afetados
        if not df_consolidado.empty and not df_novo.empty:
            df_consolidado_temp = df_consolidado.copy()
            df_consolidado_temp['mes_ano'] = df_consolidado_temp['DATA'].dt.to_period('M')
            
            df_novo_temp = df_novo.copy()
            df_novo_temp['mes_ano'] = df_novo_temp['DATA'].dt.to_period('M')
            
            df_final_temp = df_final.copy()
            df_final_temp['mes_ano'] = df_final_temp['DATA'].dt.to_period('M')
            
            # Per√≠odos que est√£o sendo atualizados
            periodos_atualizados = set()
            for responsavel in responsaveis_novos:
                periodos_resp = df_novo_temp[
                    df_novo_temp['RESPONS√ÅVEL'].astype(str).str.strip().str.upper() == responsavel
                ]['mes_ano'].unique()
                for periodo in periodos_resp:
                    periodos_atualizados.add((responsavel, periodo))
            
            # Verificar se per√≠odos n√£o atualizados foram preservados
            for responsavel in responsaveis_antes:
                if responsavel not in responsaveis_novos:
                    # Este respons√°vel n√£o est√° sendo atualizado, deve manter todos os per√≠odos
                    periodos_antes = df_consolidado_temp[
                        df_consolidado_temp['RESPONS√ÅVEL'].astype(str).str.strip().str.upper() == responsavel
                    ]['mes_ano'].unique()
                    
                    periodos_depois = df_final_temp[
                        df_final_temp['RESPONS√ÅVEL'].astype(str).str.strip().str.upper() == responsavel
                    ]['mes_ano'].unique()
                    
                    if len(periodos_antes) != len(periodos_depois):
                        error_msg = f"Per√≠odos perdidos para respons√°vel '{responsavel}': antes={len(periodos_antes)}, depois={len(periodos_depois)}"
                        logger.error(f"‚ùå ERRO: {error_msg}")
                        return False, error_msg
        
        logger.info(f"‚úÖ VERIFICA√á√ÉO DE SEGURAN√áA PASSOU!")
        logger.info(f"   Total antes: {len(responsaveis_antes)} respons√°veis")
        logger.info(f"   Total novos: {len(responsaveis_novos)} respons√°veis") 
        logger.info(f"   Total depois: {len(responsaveis_depois)} respons√°veis")
        
        return True, f"Verifica√ß√£o passou: {len(responsaveis_depois)} respons√°veis mantidos"
        
    except Exception as e:
        error_msg = f"Erro durante verifica√ß√£o de seguran√ßa: {str(e)}"
        logger.error(f"‚ùå {error_msg}")
        return False, error_msg

def comparar_e_atualizar_registros_v2(df_consolidado, df_novo):
    """
    L√≥gica de consolida√ß√£o corrigida - v2.3.0
    Consolida por RESPONS√ÅVEL + M√äS/ANO para evitar problemas com altera√ß√µes de data
    """
    registros_inseridos = 0
    registros_substituidos = 0
    registros_removidos = 0
    detalhes_operacao = []
    combinacoes_novas = 0
    combinacoes_existentes = 0
    
    logger.info(f"üîß INICIANDO CONSOLIDA√á√ÉO v2.3.0:")
    logger.info(f"   Consolidado atual: {len(df_consolidado)} registros")
    logger.info(f"   Novo arquivo: {len(df_novo)} registros")
    
    if df_consolidado.empty:
        df_final = df_novo.copy()
        registros_inseridos = len(df_novo)
        
        # Criar combina√ß√µes √∫nicas por m√™s/ano
        df_temp = df_novo.copy()
        df_temp['mes_ano'] = df_temp['DATA'].dt.to_period('M')
        combinacoes_unicas = df_temp.groupby(['RESPONS√ÅVEL', 'mes_ano']).size()
        combinacoes_novas = len(combinacoes_unicas)
        
        logger.info(f"‚úÖ PRIMEIRA CONSOLIDA√á√ÉO: {registros_inseridos} registros inseridos")
        
        for _, row in df_novo.iterrows():
            detalhes_operacao.append({
                "Opera√ß√£o": "INSERIDO",
                "Respons√°vel": row["RESPONS√ÅVEL"],
                "M√™s/Ano": row["DATA"].strftime("%m/%Y"),
                "Data": row["DATA"].strftime("%d/%m/%Y"),
                "Motivo": "Primeira consolida√ß√£o - arquivo vazio"
            })
        
        return df_final, registros_inseridos, registros_substituidos, registros_removidos, detalhes_operacao, combinacoes_novas, combinacoes_existentes
    
    # Garantir que as colunas existem no consolidado
    colunas = df_novo.columns.tolist()
    for col in colunas:
        if col not in df_consolidado.columns:
            df_consolidado[col] = None
            logger.info(f"‚ûï Coluna '{col}' adicionada ao consolidado")
    
    # Come√ßar com uma C√ìPIA COMPLETA do consolidado
    df_final = df_consolidado.copy()
    registros_inicial = len(df_final)
    
    # Adicionar colunas auxiliares para agrupamento por m√™s/ano
    df_novo_temp = df_novo.copy()
    df_novo_temp['mes_ano'] = df_novo_temp['DATA'].dt.to_period('M')
    
    df_final_temp = df_final.copy()
    df_final_temp['mes_ano'] = df_final_temp['DATA'].dt.to_period('M')
    
    logger.info(f"üìã Estado inicial do consolidado:")
    if not df_final.empty:
        responsaveis_iniciais = df_final['RESPONS√ÅVEL'].dropna().unique()
        logger.info(f"   Respons√°veis: {responsaveis_iniciais}")
        logger.info(f"   Total de registros: {len(df_final)}")
    
    # Agrupar registros novos por RESPONS√ÅVEL e M√äS/ANO
    grupos_novos = df_novo_temp.groupby(['RESPONS√ÅVEL', 'mes_ano'])
    
    logger.info(f"üìä Processando {len(grupos_novos)} combina√ß√µes √∫nicas de Respons√°vel+M√™s/Ano")
    
    for (responsavel, periodo_grupo), grupo_df in grupos_novos:
        if pd.isna(responsavel) or str(responsavel).strip() == '':
            logger.warning(f"‚ö†Ô∏è Pulando respons√°vel inv√°lido: {responsavel}")
            continue
        
        logger.info(f"üîç Processando: '{responsavel}' em {periodo_grupo} ({len(grupo_df)} registros)")
        
        # Buscar registros existentes APENAS para este respons√°vel e per√≠odo ESPEC√çFICOS
        mask_existente = (
            (df_final_temp["mes_ano"] == periodo_grupo) &
            (df_final_temp["RESPONS√ÅVEL"].astype(str).str.strip().str.upper() == str(responsavel).strip().upper())
        )
        
        registros_existentes = df_final[mask_existente]
        total_antes_operacao = len(df_final)
        
        logger.info(f"   üìã Encontrados {len(registros_existentes)} registros existentes para esta combina√ß√£o")
        
        if not registros_existentes.empty:
            # SUBSTITUI√á√ÉO APENAS DA COMBINA√á√ÉO ESPEC√çFICA (RESPONS√ÅVEL + M√äS/ANO)
            num_removidos = len(registros_existentes)
            
            logger.info(f"   üîÑ SUBSTITUI√á√ÉO: Removendo {num_removidos} registros antigos do per√≠odo {periodo_grupo}")
            
            # Remove APENAS os registros dessa combina√ß√£o espec√≠fica
            df_final = df_final[~mask_existente]
            
            # Atualizar df_final_temp tamb√©m
            df_final_temp = df_final_temp[~mask_existente]
            
            total_depois_remocao = len(df_final)
            registros_removidos += num_removidos
            combinacoes_existentes += 1
            
            logger.info(f"   ‚úÖ Removidos {num_removidos} registros. Total: {total_antes_operacao} -> {total_depois_remocao}")
            
            # Verifica√ß√£o de seguran√ßa na remo√ß√£o
            if total_depois_remocao != (total_antes_operacao - num_removidos):
                logger.error(f"‚ùå ERRO NA REMO√á√ÉO! Esperado: {total_antes_operacao - num_removidos}, Atual: {total_depois_remocao}")
            
            # Adicionar detalhes da remo√ß√£o
            detalhes_operacao.append({
                "Opera√ß√£o": "REMOVIDO",
                "Respons√°vel": responsavel,
                "M√™s/Ano": periodo_grupo.strftime("%m/%Y"),
                "Data": f"Todo o per√≠odo {periodo_grupo}",
                "Motivo": f"Substitui√ß√£o: {num_removidos} registro(s) antigo(s) removido(s)"
            })
            
            registros_substituidos += len(grupo_df)
            operacao_tipo = "SUBSTITU√çDO"
            motivo = f"Substitui√ß√£o completa do per√≠odo: {len(grupo_df)} novo(s) registro(s)"
            
        else:
            # INSER√á√ÉO DE NOVOS DADOS
            logger.info(f"   ‚ûï NOVA COMBINA√á√ÉO: Adicionando {len(grupo_df)} registros para {periodo_grupo}")
            registros_inseridos += len(grupo_df)
            combinacoes_novas += 1
            operacao_tipo = "INSERIDO"
            motivo = f"Nova combina√ß√£o: {len(grupo_df)} registro(s) inserido(s)"
        
        # Inserir novos registros (tanto para inser√ß√£o quanto substitui√ß√£o)
        total_antes_insercao = len(df_final)
        
        # Remover coluna auxiliar antes de concatenar
        grupo_para_inserir = grupo_df.drop(columns=['mes_ano'], errors='ignore')
        df_final = pd.concat([df_final, grupo_para_inserir], ignore_index=True)
        
        # Atualizar df_final_temp tamb√©m
        df_final_temp = pd.concat([df_final_temp, grupo_df], ignore_index=True)
        
        total_depois_insercao = len(df_final)
        
        logger.info(f"   ‚úÖ Inseridos {len(grupo_df)} registros. Total: {total_antes_insercao} -> {total_depois_insercao}")
        
        # Adicionar detalhes da opera√ß√£o
        detalhes_operacao.append({
            "Opera√ß√£o": operacao_tipo,
            "Respons√°vel": responsavel,
            "M√™s/Ano": periodo_grupo.strftime("%m/%Y"),
            "Data": f"{grupo_df['DATA'].min().strftime('%d/%m/%Y')} at√© {grupo_df['DATA'].max().strftime('%d/%m/%Y')}",
            "Motivo": motivo
        })
    
    # Log final detalhado
    logger.info(f"üèÅ CONSOLIDA√á√ÉO FINALIZADA:")
    logger.info(f"   Total inicial: {registros_inicial}")
    logger.info(f"   Total final: {len(df_final)}")
    logger.info(f"   Inseridos: {registros_inseridos}")
    logger.info(f"   Substitu√≠dos: {registros_substituidos}")
    logger.info(f"   Removidos: {registros_removidos}")
    
    if not df_final.empty:
        responsaveis_finais = df_final['RESPONS√ÅVEL'].dropna().unique()
        logger.info(f"   Respons√°veis finais: {responsaveis_finais}")
    
    return df_final, registros_inseridos, registros_substituidos, registros_removidos, detalhes_operacao, combinacoes_novas, combinacoes_existentes

def analise_pre_consolidacao_v2(df_consolidado, df_novo):
    """
    An√°lise pr√©-consolida√ß√£o corrigida para trabalhar com m√™s/ano
    """
    try:
        responsaveis_no_envio = df_novo["RESPONS√ÅVEL"].dropna().unique()
        periodo_min = df_novo["DATA"].min().strftime("%d/%m/%Y")
        periodo_max = df_novo["DATA"].max().strftime("%d/%m/%Y")
        
        # Criar combina√ß√µes por m√™s/ano
        df_novo_temp = df_novo.copy()
        df_novo_temp['mes_ano'] = df_novo_temp['DATA'].dt.to_period('M')
        combinacoes_envio = df_novo_temp.groupby(['RESPONS√ÅVEL', 'mes_ano']).size()
        total_combinacoes = len(combinacoes_envio)
        
        st.markdown("### üîç **An√°lise Pr√©-Consolida√ß√£o (Por M√™s/Ano)**")
        
        col1, col2, col3 = st.columns(3)
        with col1:
            st.info(f"üë• **Respons√°veis no envio:**\n{', '.join(responsaveis_no_envio)}")
        with col2:
            st.info(f"üìÖ **Per√≠odo:**\n{periodo_min} at√© {periodo_max}")
        with col3:
            st.info(f"üìä **Combina√ß√µes √∫nicas:**\n{total_combinacoes} (Respons√°vel + M√™s/Ano)")
        
        if not df_consolidado.empty:
            # Criar coluna auxiliar para o consolidado
            df_consolidado_temp = df_consolidado.copy()
            df_consolidado_temp['mes_ano'] = df_consolidado_temp['DATA'].dt.to_period('M')
            
            registros_para_consolidar = 0
            registros_para_alterar = 0
            registros_que_serao_removidos = 0
            periodos_afetados = []
            
            for responsavel in responsaveis_no_envio:
                periodos_envio = df_novo_temp[df_novo_temp["RESPONS√ÅVEL"] == responsavel]["mes_ano"].unique()
                
                for periodo in periodos_envio:
                    mask_conflito = (
                        (df_consolidado_temp["mes_ano"] == periodo) &
                        (df_consolidado_temp["RESPONS√ÅVEL"].astype(str).str.strip().str.upper() == str(responsavel).strip().upper())
                    )
                    
                    registros_envio = len(df_novo_temp[
                        (df_novo_temp["RESPONS√ÅVEL"] == responsavel) & 
                        (df_novo_temp["mes_ano"] == periodo)
                    ])
                    
                    if mask_conflito.any():
                        registros_existentes = mask_conflito.sum()
                        registros_que_serao_removidos += registros_existentes
                        registros_para_alterar += registros_envio
                        periodos_afetados.append(f"{responsavel} - {periodo}")
                    else:
                        registros_para_consolidar += registros_envio
            
            total_atual = len(df_consolidado)
            total_enviado = len(df_novo)
            total_esperado = total_atual - registros_que_serao_removidos + total_enviado
            
            st.markdown("### üìà **Impacto da Consolida√ß√£o**")
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìä Registros Atuais", f"{total_atual:,}")
            with col2:
                st.metric("üì§ Registros Enviados", f"{total_enviado:,}")
            with col3:
                st.metric("üóëÔ∏è Ser√£o Removidos", f"{registros_que_serao_removidos:,}")
            with col4:
                st.metric("üéØ Total Esperado", f"{total_esperado:,}")
            
            if periodos_afetados:
                st.warning("‚ö†Ô∏è **Per√≠odos que ser√£o substitu√≠dos:**")
                for periodo in periodos_afetados[:10]:  # Mostrar apenas os primeiros 10
                    st.caption(f"üîÑ {periodo}")
                if len(periodos_afetados) > 10:
                    st.caption(f"... e mais {len(periodos_afetados) - 10} per√≠odos")
            
            if registros_para_consolidar > 0 and registros_para_alterar == 0:
                st.success(f"‚úÖ **{registros_para_consolidar} registro(s) ser√£o CONSOLIDADOS** (dados novos)")
                st.info("‚ÑπÔ∏è Nenhum per√≠odo existente ser√° alterado")
                
            elif registros_para_alterar > 0 and registros_para_consolidar == 0:
                st.warning(f"üîÑ **{registros_para_alterar} registro(s) ser√£o ALTERADOS** (substituindo per√≠odos existentes)")
                st.info("‚ÑπÔ∏è Nenhum per√≠odo novo ser√° adicionado")
                
            elif registros_para_consolidar > 0 and registros_para_alterar > 0:
                col1, col2 = st.columns(2)
                with col1:
                    st.success(f"‚úÖ **{registros_para_consolidar} registro(s) ser√£o CONSOLIDADOS**")
                    st.caption("(per√≠odos completamente novos)")
                with col2:
                    st.warning(f"üîÑ **{registros_para_alterar} registro(s) ser√£o ALTERADOS**")
                    st.caption("(substituindo per√≠odos existentes)")
            
            if registros_que_serao_removidos > 0:
                st.warning(f"‚ö†Ô∏è **{registros_que_serao_removidos} registros existentes ser√£o substitu√≠dos** pelos novos dados")
                st.info("üíæ Um backup autom√°tico ser√° criado dos dados substitu√≠dos")
                st.info("üóìÔ∏è **IMPORTANTE:** A substitui√ß√£o √© feita por **RESPONS√ÅVEL + M√äS/ANO** completo")
        else:
            st.success(f"‚úÖ **{len(df_novo)} registro(s) ser√£o CONSOLIDADOS** (primeira consolida√ß√£o)")
            
        return True
        
    except Exception as e:
        st.error(f"‚ùå Erro na an√°lise pr√©-consolida√ß√£o: {e}")
        logger.error(f"Erro na an√°lise pr√©-consolida√ß√£o: {e}")
        return False

def salvar_arquivo_enviado(df, nome_arquivo_original, token):
    """Salva o arquivo enviado"""
    try:
        if not df.empty and "DATA" in df.columns:
            data_base = df["DATA"].min()
            nome_pasta = f"Relatorios_Enviados/{data_base.strftime('%Y-%m')}"
            timestamp = datetime.now().strftime('%d-%m-%Y_%Hh%M')
            
            nome_sem_extensao = os.path.splitext(nome_arquivo_original)[0]
            nome_arquivo = f"{nome_pasta}/{nome_sem_extensao}_{timestamp}_v{APP_VERSION}.xlsx"
            
            buffer_envio = BytesIO()
            with pd.ExcelWriter(buffer_envio, engine='openpyxl') as writer:
                df.to_excel(writer, index=False, sheet_name="Vendas CTs")
            buffer_envio.seek(0)
            
            sucesso, _, _ = upload_onedrive(nome_arquivo, buffer_envio.read(), token, "enviado")
            if sucesso:
                st.info(f"üíæ Arquivo salvo como: {PASTA_ENVIOS_BACKUPS}/{nome_arquivo}")
            else:
                st.warning("‚ö†Ô∏è N√£o foi poss√≠vel salvar c√≥pia do arquivo enviado")
                
    except Exception as e:
        st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel salvar c√≥pia do arquivo: {e}")
        logger.error(f"Erro ao salvar arquivo enviado: {e}")

def processar_consolidacao_com_lock(df_novo, nome_arquivo, token):
    """Consolida√ß√£o com sistema de lock e feedback melhorado - v2.3.0"""
    session_id = gerar_id_sessao()
    
    status_container = st.empty()
    progress_container = st.empty()
    
    try:
        status_container.info("üìÑ **Iniciando processo de consolida√ß√£o v2.3.0...**")
        
        sistema_ocupado, lock_data = verificar_lock_existente(token)
        if sistema_ocupado:
            status_container.error("üîí Sistema ocupado! Outro usu√°rio est√° fazendo consolida√ß√£o.")
            return False
        
        status_container.info("üîí **Bloqueando sistema para consolida√ß√£o...**")
        progress_container.progress(10)
        
        lock_criado, session_lock = criar_lock(token, "Consolida√ß√£o de planilha v2.3.0")
        
        if not lock_criado:
            status_container.error("‚ùå N√£o foi poss√≠vel bloquear o sistema. Tente novamente.")
            return False
        
        status_container.success(f"‚úÖ **Sistema bloqueado com sucesso!** (ID: {session_lock})")
        progress_container.progress(15)
        
        atualizar_status_lock(token, session_lock, "BAIXANDO_ARQUIVO", "Baixando arquivo consolidado")
        status_container.info("üì• **Baixando arquivo consolidado existente...**")
        progress_container.progress(25)
        
        df_consolidado, arquivo_existe = baixar_arquivo_consolidado(token)
        
        if arquivo_existe:
            status_container.info(f"üìÇ **Arquivo consolidado carregado** ({len(df_consolidado):,} registros)")
        else:
            status_container.info("üìÇ **Criando novo arquivo consolidado**")
        
        progress_container.progress(35)

        atualizar_status_lock(token, session_lock, "PREPARANDO_DADOS", "Validando e preparando dados")
        status_container.info("üîß **Preparando e validando dados...**")
        
        df_novo = df_novo.copy()
        df_novo.columns = df_novo.columns.str.strip().str.upper()
        
        df_novo["DATA"] = pd.to_datetime(df_novo["DATA"], errors="coerce")
        linhas_invalidas = df_novo["DATA"].isna().sum()
        df_novo = df_novo.dropna(subset=["DATA"])

        if df_novo.empty:
            status_container.error("‚ùå **Nenhum registro v√°lido para consolidar**")
            remover_lock(token, session_lock)
            return False

        if linhas_invalidas > 0:
            status_container.warning(f"üßπ **{linhas_invalidas} linhas com datas inv√°lidas foram removidas**")

        progress_container.progress(45)

        status_container.info("üìä **Realizando an√°lise pr√©-consolida√ß√£o...**")
        analise_ok = analise_pre_consolidacao_v2(df_consolidado, df_novo)
        
        if not analise_ok:
            status_container.error("‚ùå **Erro na an√°lise pr√©-consolida√ß√£o**")
            remover_lock(token, session_lock)
            return False
        
        progress_container.progress(55)

        atualizar_status_lock(token, session_lock, "CONSOLIDANDO", f"Processando {len(df_novo)} registros por m√™s/ano")
        status_container.info("üîÑ **Processando consolida√ß√£o (l√≥gica por m√™s/ano v2.3.0)...**")
        progress_container.progress(65)
        
        df_final, inseridos, substituidos, removidos, detalhes, novas_combinacoes, combinacoes_existentes = comparar_e_atualizar_registros_v2(
            df_consolidado, df_novo
        )
        
        progress_container.progress(75)

        status_container.info("üõ°Ô∏è **Executando verifica√ß√£o de seguran√ßa...**")
        verificacao_ok, msg_verificacao = verificar_seguranca_consolidacao_v2(df_consolidado, df_novo, df_final)
        
        if not verificacao_ok:
            status_container.error(f"‚ùå **ERRO DE SEGURAN√áA:** {msg_verificacao}")
            st.error("üõë **Consolida√ß√£o cancelada para proteger os dados!**")
            remover_lock(token, session_lock)
            return False
        else:
            status_container.success(f"‚úÖ **Verifica√ß√£o de seguran√ßa passou:** {msg_verificacao}")

        df_final = df_final.sort_values(["DATA", "RESPONS√ÅVEL"], na_position='last').reset_index(drop=True)
        progress_container.progress(80)
        
        if removidos > 0:
            atualizar_status_lock(token, session_lock, "CRIANDO_BACKUP", f"Backup de {removidos} registros substitu√≠dos")
            status_container.info("üíæ **Criando backup dos dados substitu√≠dos...**")
        
        atualizar_status_lock(token, session_lock, "SALVANDO_ENVIADO", "Salvando c√≥pia do arquivo enviado")
        status_container.info("üíæ **Salvando c√≥pia do arquivo enviado...**")
        salvar_arquivo_enviado(df_novo, nome_arquivo, token)
        
        progress_container.progress(85)
        
        atualizar_status_lock(token, session_lock, "UPLOAD_FINAL", "Salvando arquivo consolidado")
        status_container.info("üì§ **Salvando arquivo consolidado final...**")
        
        buffer = BytesIO()
        with pd.ExcelWriter(buffer, engine='openpyxl') as writer:
            df_final.to_excel(writer, index=False, sheet_name="Vendas CTs")
        buffer.seek(0)
        
        consolidado_nome = "Reports_Geral_Consolidado.xlsx"
        sucesso, status_code, resposta = upload_onedrive(consolidado_nome, buffer.read(), token, "consolidado")

        progress_container.progress(95)

        remover_lock(token, session_lock)
        progress_container.progress(100)
        
        if sucesso:
            status_container.empty()
            progress_container.empty()
            
            st.success("üéâ **CONSOLIDA√á√ÉO REALIZADA COM SUCESSO!**")
            st.success("üîì **Sistema liberado e dispon√≠vel para outros usu√°rios**")
            
            with st.expander("üìç Localiza√ß√£o dos Arquivos", expanded=True):
                col1, col2 = st.columns(2)
                with col1:
                    st.info(f"üìä **Arquivo Consolidado:**\n`{PASTA_CONSOLIDADO}/Reports_Geral_Consolidado.xlsx`")
                with col2:
                    st.info(f"üíæ **Backups e Envios:**\n`{PASTA_ENVIOS_BACKUPS}/`")
            
            st.markdown("### üìà **Resultado da Consolida√ß√£o**")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìä Total Final", f"{len(df_final):,}")
            with col2:
                st.metric("‚ûï Inseridos", f"{inseridos}")
            with col3:
                st.metric("üîÑ Substitu√≠dos", f"{substituidos}")
            with col4:
                st.metric("üóëÔ∏è Removidos", f"{removidos}")
            
            if novas_combinacoes > 0 or combinacoes_existentes > 0:
                st.markdown("### üìà **An√°lise de Combina√ß√µes (Respons√°vel + M√™s/Ano)**")
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("üÜï Novos Per√≠odos", novas_combinacoes, 
                             help="Combina√ß√µes de Respons√°vel + M√™s/Ano que n√£o existiam antes")
                with col2:
                    st.metric("üîÑ Per√≠odos Atualizados", combinacoes_existentes,
                             help="Per√≠odos que j√° existiam e foram substitu√≠dos completamente")
                with col3:
                    total_processadas = novas_combinacoes + combinacoes_existentes
                    st.metric("üìä Total Processado", total_processadas,
                             help="Total de per√≠odos mensais processados")
                
                if novas_combinacoes > 0:
                    st.success(f"üéâ **{novas_combinacoes} novo(s) per√≠odo(s) adicionado(s)** - Dados completamente novos!")
                if combinacoes_existentes > 0:
                    st.info(f"üîÑ **{combinacoes_existentes} per√≠odo(s) atualizado(s)** - Dados mensais completamente substitu√≠dos!")
            
            if detalhes:
                with st.expander("üìã Detalhes das Opera√ß√µes", expanded=removidos > 0):
                    df_detalhes = pd.DataFrame(detalhes)
                    
                    operacoes_inseridas = df_detalhes[df_detalhes['Opera√ß√£o'] == 'INSERIDO']
                    operacoes_substituidas = df_detalhes[df_detalhes['Opera√ß√£o'] == 'SUBSTITU√çDO']
                    operacoes_removidas = df_detalhes[df_detalhes['Opera√ß√£o'] == 'REMOVIDO']
                    
                    if not operacoes_inseridas.empty:
                        st.markdown("#### ‚ûï **Registros Inseridos (Novos)**")
                        st.dataframe(operacoes_inseridas, use_container_width=True, hide_index=True)
                    
                    if not operacoes_substituidas.empty:
                        st.markdown("#### üîÑ **Registros Substitu√≠dos**")
                        st.dataframe(operacoes_substituidas, use_container_width=True, hide_index=True)
                    
                    if not operacoes_removidas.empty:
                        st.markdown("#### üóëÔ∏è **Registros Removidos**")
                        st.dataframe(operacoes_removidas, use_container_width=True, hide_index=True)
            
            if not df_final.empty:
                resumo_responsaveis = df_final.groupby("RESPONS√ÅVEL").agg({
                    "DATA": ["count", "min", "max"]
                }).round(0)
                
                resumo_responsaveis.columns = ["Total Registros", "Data Inicial", "Data Final"]
                resumo_responsaveis["Data Inicial"] = pd.to_datetime(resumo_responsaveis["Data Inicial"]).dt.strftime("%d/%m/%Y")
                resumo_responsaveis["Data Final"] = pd.to_datetime(resumo_responsaveis["Data Final"]).dt.strftime("%d/%m/%Y")
                
                with st.expander("üë• Resumo por Respons√°vel"):
                    st.dataframe(resumo_responsaveis, use_container_width=True)
            
            return True
        else:
            status_container.error(f"‚ùå **Erro no upload:** Status {status_code}")
            if status_code != 500:
                st.code(resposta)
            return False
            
    except Exception as e:
        logger.error(f"Erro na consolida√ß√£o: {e}")
        remover_lock(token, session_id, force=True)
        
        status_container.error(f"‚ùå **Erro durante consolida√ß√£o:** {str(e)}")
        progress_container.empty()
        st.error("üîì **Sistema liberado automaticamente ap√≥s erro.**")
        return False

# ===========================
# INTERFACE STREAMLIT
# ===========================
def exibir_info_versao():
    """Exibe informa√ß√µes de vers√£o e changelog"""
    with st.sidebar:
        st.markdown("---")
        st.markdown("### ‚ÑπÔ∏è Informa√ß√µes do Sistema")
        st.info(f"**Vers√£o:** {APP_VERSION}")
        st.info(f"**Data:** {VERSION_DATE}")
        
        if APP_VERSION == "2.3.0":
            st.success("üéØ **L√ìGICA POR M√äS/ANO**")
            st.caption("Resolve problema de duplicatas")
        
        with st.expander("üìù Configura√ß√£o de Pastas"):
            st.markdown("**Arquivo Consolidado:**")
            st.code(PASTA_CONSOLIDADO, language=None)
            st.markdown("**Backups e Envios:**")
            st.code(PASTA_ENVIOS_BACKUPS, language=None)

def main():
    st.set_page_config(
        page_title=f"DSView BI - Upload Planilhas v{APP_VERSION}", 
        layout="wide",
        initial_sidebar_state="expanded"
    )

    st.markdown(
        f'''
        <div style="display: flex; align-items: center; justify-content: space-between; margin-bottom: 20px;">
            <div style="display: flex; align-items: center; gap: 15px;">
                <h2 style="margin: 0; color: #2E8B57;">üìä DSView BI ‚Äî Upload de Planilhas</h2>
            </div>
            <div style="text-align: right; color: #666; font-size: 0.9em;">
                <strong>v{APP_VERSION}</strong><br>
                <small>{VERSION_DATE}</small>
            </div>
        </div>
        ''',
        unsafe_allow_html=True
    )

    if APP_VERSION == "2.3.0":
        st.success("üéØ **L√ìGICA DE CONSOLIDA√á√ÉO POR M√äS/ANO** - Problema de duplicatas por altera√ß√£o de datas resolvido!")

    st.sidebar.markdown("### üì§ Upload de Planilhas")
    st.sidebar.markdown("Sistema de consolida√ß√£o de relat√≥rios")
    st.sidebar.divider()
    st.sidebar.markdown("**Status do Sistema:**")
    
    token = obter_token()
    if not token:
        st.sidebar.error("‚ùå Desconectado")
        st.error("‚ùå N√£o foi poss√≠vel autenticar. Verifique as credenciais.")
        st.stop()
    else:
        st.sidebar.success("‚úÖ Conectado")

    st.markdown("## üîí Status do Sistema")
    
    sistema_ocupado = exibir_status_sistema(token)
    
    if sistema_ocupado:
        st.markdown("---")
        st.info("üîÑ Esta p√°gina ser√° atualizada automaticamente a cada 15 segundos")
        time.sleep(15)
        st.rerun()

    st.divider()

    exibir_info_versao()

    st.markdown("## üì§ Upload de Planilha Excel")
    
    if sistema_ocupado:
        st.warning("‚ö†Ô∏è **Upload desabilitado** - Sistema em uso por outro usu√°rio")
        st.info("üí° **Aguarde** a libera√ß√£o do sistema ou tente novamente em alguns minutos")
        
        if st.button("üîÑ Verificar Status Novamente"):
            st.rerun()
        
        return
    
    st.info("üí° **Importante**: A planilha deve conter uma coluna 'RESPONS√ÅVEL' com os nomes dos respons√°veis!")
    
    st.error("üîí **VALIDA√á√ÉO SUPER RIGOROSA ATIVADA**")
    st.warning("üìã **QUALQUER problema de data (vazias, formato inv√°lido, futuras, antigas) impedir√° a consolida√ß√£o!**")
    st.info("üí° **Dica**: Revise cuidadosamente sua planilha antes de enviar. Todas as datas devem estar corretas.")
    
    with st.expander("üéØ Corre√ß√µes da v2.3.0 - L√ìGICA POR M√äS/ANO", expanded=True):
        st.markdown("### üõ°Ô∏è **Problemas Corrigidos:**")
        
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("""
            **‚ùå Problema Anterior:**
            - Consolida√ß√£o por data exata (dia espec√≠fico)
            - Altera√ß√£o de datas criava duplicatas
            - Inconsist√™ncias no mesmo per√≠odo mensal
            - Dados duplicados para o mesmo respons√°vel
            """)
            
        with col2:
            st.markdown("""
            **‚úÖ Solu√ß√µes Implementadas:**
            - Consolida√ß√£o por RESPONS√ÅVEL + M√äS/ANO
            - Substitui√ß√£o completa do per√≠odo mensal
            - Verifica√ß√£o de seguran√ßa por per√≠odos
            - Elimina√ß√£o total de duplicatas temporais
            """)
        
        st.success("üéØ **Resultado:** Consolida√ß√£o 100% consistente - altera√ß√µes de data dentro do mesmo m√™s substituem todos os registros daquele per√≠odo!")
        
        st.markdown("### üìÖ **Como Funciona Agora:**")
        st.info("Se Jo√£o tinha dados de 10/03/2024 e voc√™ envia dados de 15/03/2024, TODOS os registros de Jo√£o em mar√ßo/2024 s√£o substitu√≠dos pelos novos dados")
        st.info("Isso garante que n√£o haver√° dados duplicados ou inconsistentes para o mesmo respons√°vel no mesmo m√™s")
    
    st.divider()

    uploaded_file = st.file_uploader(
        "Escolha um arquivo Excel", 
        type=["xlsx", "xls"],
        help="Formatos aceitos: .xlsx, .xls | Certifique-se de que h√° uma coluna 'RESPONS√ÅVEL' na planilha"
    )

    df = None
    if uploaded_file:
        try:
            st.success(f"üìÅ Arquivo carregado: **{uploaded_file.name}**")
            
            file_extension = uploaded_file.name.split('.')[-1].lower()
            
            with st.spinner("üìñ Lendo arquivo..."):
                if file_extension == 'xls':
                    xls = pd.ExcelFile(uploaded_file, engine='xlrd')
                else:
                    xls = pd.ExcelFile(uploaded_file)
                
                sheets = xls.sheet_names
                
                if len(sheets) > 1:
                    if "Vendas CTs" in sheets:
                        sheet = "Vendas CTs"
                        st.success("‚úÖ Aba 'Vendas CTs' encontrada e selecionada automaticamente")
                    else:
                        sheet = st.selectbox(
                            "Selecione a aba (recomendado: 'Vendas CTs'):", 
                            sheets,
                            help="Para melhor compatibilidade, use uma aba chamada 'Vendas CTs'"
                        )
                        if sheet != "Vendas CTs":
                            st.warning("‚ö†Ô∏è Recomendamos que a aba seja chamada 'Vendas CTs'")
                else:
                    sheet = sheets[0]
                    if sheet != "Vendas CTs":
                        st.warning(f"‚ö†Ô∏è A aba atual se chama '{sheet}'. Recomendamos renome√°-la para 'Vendas CTs'")
                
                df = pd.read_excel(uploaded_file, sheet_name=sheet)
                df.columns = df.columns.str.strip().str.upper()
                
            st.success(f"‚úÖ Dados carregados com sucesso!")
            
        except Exception as e:
            st.error(f"‚ùå Erro ao ler o Excel: {e}")
            logger.error(f"Erro ao ler Excel: {e}")

    if df is not None:
        st.subheader("üëÄ Pr√©via dos dados")
        st.dataframe(df.head(10), use_container_width=True, height=300)

        st.subheader("üìä Resumo dos dados")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Linhas", df.shape[0])
        with col2:
            st.metric("Colunas", df.shape[1])
        with col3:
            if "DATA" in df.columns:
                datas_validas = pd.to_datetime(df["DATA"], errors="coerce").notna().sum()
                st.metric("Datas v√°lidas", datas_validas)

        st.subheader("üí∞ Resumo de Totais por Produto")
        
        colunas_produtos = ['TMO - DUTO', 'TMO - FREIO', 'TMO - SANIT', 'TMO - VERNIZ', 'CX EVAP']
        colunas_encontradas = [col for col in colunas_produtos if col in df.columns]
        
        if colunas_encontradas:
            totais = {}
            total_geral = 0
            
            for coluna in colunas_encontradas:
                valores_numericos = pd.to_numeric(df[coluna], errors='coerce').fillna(0)
                total = int(valores_numericos.sum())
                totais[coluna] = total
                total_geral += total
            
            colunas_tmo = [col for col in colunas_encontradas if col.startswith('TMO -')]
            if colunas_tmo:
                tmo_total = sum(totais[col] for col in colunas_tmo)
                totais['TMO - TOTAL'] = tmo_total
            
            produtos_para_exibir = [col for col in colunas_produtos if col in totais]
            if 'TMO - TOTAL' in totais:
                produtos_para_exibir.append('TMO - TOTAL')
            
            num_colunas = len(produtos_para_exibir)
            cols = st.columns(num_colunas)
            
            for i, coluna in enumerate(produtos_para_exibir):
                with cols[i]:
                    total = totais[coluna]
                    total_formatado = f"{total:,}".replace(',', '.')
                    
                    if 'DUTO' in coluna:
                        emoji = "üîß"
                    elif 'FREIO' in coluna:
                        emoji = "üöó"
                    elif 'SANIT' in coluna:
                        emoji = "üßΩ"
                    elif 'VERNIZ' in coluna:
                        emoji = "üé®"
                    elif 'EVAP' in coluna:
                        emoji = "üì¶"
                    elif 'TOTAL' in coluna:
                        emoji = "üí∞"
                    else:
                        emoji = "üìä"
                    
                    nome_display = coluna.replace('TMO - ', '').title()
                    
                    st.metric(f"{emoji} {nome_display}", total_formatado)
        else:
            st.warning("‚ö†Ô∏è Nenhuma coluna de produtos encontrada")

        colunas_nulas = df.columns[df.isnull().any()].tolist()
        if colunas_nulas:
            st.warning(f"‚ö†Ô∏è Colunas com valores nulos: {', '.join(colunas_nulas[:5])}")
            if len(colunas_nulas) > 5:
                st.warning(f"... e mais {len(colunas_nulas) - 5} colunas")
        else:
            st.success("‚úÖ Nenhuma coluna com valores nulos.")

        st.subheader("üîç Valida√ß√µes Super Rigorosas")
        erros, avisos, linhas_invalidas_detalhes = validar_dados_enviados(df)
        
        for aviso in avisos:
            if aviso.startswith("‚úÖ"):
                st.success(aviso)
            else:
                st.warning(aviso)
        
        if linhas_invalidas_detalhes:
            exibir_relatorio_problemas_datas(linhas_invalidas_detalhes)
        
        if erros:
            st.markdown("## ‚ùå **PROBLEMAS ENCONTRADOS - CORRE√á√ÉO OBRIGAT√ìRIA**")
            
            for erro in erros:
                if erro.startswith("‚ùå"):
                    st.error(erro)
                elif erro.startswith("üîß"):
                    st.error(erro)
                elif erro.startswith("üìã"):
                    st.warning(erro)
                else:
                    st.error(erro)
            
            st.markdown("---")
            st.error("üö´ **A consolida√ß√£o est√° BLOQUEADA at√© que todos os problemas sejam corrigidos!**")
            st.info("üí° **Pr√≥ximos passos:**")
            st.info("1. ‚úèÔ∏è Abra sua planilha Excel")
            st.info("2. üîß Corrija TODOS os problemas listados acima")
            st.info("3. üíæ Salve o arquivo")
            st.info("4. üîÑ Fa√ßa o upload novamente")

        st.divider()
        st.markdown("### üöÄ **Consolidar Dados**")
        
        # Verificar novamente se sistema est√° livre antes de permitir envio
        sistema_ocupado_agora, _ = verificar_lock_existente(token)
        
        if sistema_ocupado_agora:
            st.error("üîí Sistema foi bloqueado por outro usu√°rio")
            if st.button("üîÑ Atualizar P√°gina"):
                st.rerun()
        else:
            # Bot√£o desabilitado se houver QUALQUER erro
            botao_desabilitado = bool(erros)
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                if botao_desabilitado:
                    st.button("‚ùå Consolidar Dados", type="primary", disabled=True, 
                             help="Corrija todos os problemas antes de prosseguir")
                    st.caption("üîí Bot√£o bloqueado - h√° problemas na planilha")
                else:
                    # Bot√£o principal sem confirma√ß√£o dupla
                    if st.button("‚úÖ **Consolidar Dados**", type="primary", 
                                help="Inicia a consolida√ß√£o por m√™s/ano imediatamente"):
                        
                        # Aviso importante antes de iniciar
                        st.warning("‚è≥ **Consolida√ß√£o iniciada! Aguarde o t√©rmino do processo. N√ÉO feche esta p√°gina!**")
                        
                        # Iniciar consolida√ß√£o diretamente
                        sucesso = processar_consolidacao_com_lock(df, uploaded_file.name, token)
                        
                        if sucesso:
                            st.balloons()
                            st.success("üéâ **CONSOLIDA√á√ÉO FINALIZADA COM SUCESSO!**")
                            st.info("üí° Voc√™ pode enviar uma nova planilha ou fechar esta p√°gina")
                        else:
                            st.error("‚ùå **Falha na consolida√ß√£o. Tente novamente.**")
            
            with col2:
                if st.button("üîÑ Limpar Tela", type="secondary"):
                    st.rerun()
                    
        # Informa√ß√µes sobre o que a consolida√ß√£o far√°
        with st.expander("‚ÑπÔ∏è O que acontecer√° durante a consolida√ß√£o?", expanded=False):
            st.info("**üìä An√°lise dos dados enviados por m√™s/ano**")
            st.info("**üîÑ Substitui√ß√£o de per√≠odos mensais existentes** (mesmo respons√°vel + m√™s/ano)")
            st.info("**‚ûï Adi√ß√£o de novos per√≠odos** (combina√ß√µes inexistentes)")
            st.info("**üíæ Cria√ß√£o de backups autom√°ticos** dos dados substitu√≠dos")
            st.info("**üîí Bloqueio tempor√°rio do sistema** durante o processo")
            st.info("**üõ°Ô∏è Verifica√ß√£o de seguran√ßa** antes de salvar")
            st.info("**üìà Relat√≥rio completo** das opera√ß√µes realizadas")
            st.success("**üéØ NOVO:** Agora a consolida√ß√£o √© feita por **RESPONS√ÅVEL + M√äS/ANO** - elimina duplicatas!")

    st.divider()
    st.markdown(
        f"""
        <div style="text-align: center; color: #666; font-size: 0.8em;">
            <strong>DSView BI - Sistema de Consolida√ß√£o de Relat√≥rios v{APP_VERSION}</strong><br>
            ‚ö†Ô∏è Certifique-se de que sua planilha contenha:<br>
            ‚Ä¢ Uma aba chamada <strong>'Vendas CTs'</strong><br>
            ‚Ä¢ Uma coluna <strong>'DATA'</strong><br>
            ‚Ä¢ Uma coluna <strong>'RESPONS√ÅVEL'</strong><br>
            ‚Ä¢ Colunas: <strong>TMO - Duto, TMO - Freio, TMO - Sanit, TMO - Verniz, CX EVAP</strong><br>
            <br>
            üéØ <strong>v2.3.0:</strong> L√≥gica de consolida√ß√£o por m√™s/ano - Duplicatas eliminadas<br>
            <small>√öltima atualiza√ß√£o: {VERSION_DATE}</small>
        </div>
        """,
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()